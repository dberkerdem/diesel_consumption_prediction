{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date, timedelta, datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4698 entries, 15 to 5912\n",
      "Data columns (total 18 columns):\n",
      " #   Column                              Non-Null Count  Dtype         \n",
      "---  ------                              --------------  -----         \n",
      " 0   date                                4698 non-null   datetime64[ns]\n",
      " 1   province                            4698 non-null   object        \n",
      " 2   current_month_consumption           4698 non-null   float64       \n",
      " 3   last_year_same_month_consumption    4698 non-null   float64       \n",
      " 4   last_year_total_consumption         4698 non-null   float64       \n",
      " 5   current_month_share                 4698 non-null   float64       \n",
      " 6   previous_1_month_share              4698 non-null   float64       \n",
      " 7   previous_2_month_share              4698 non-null   float64       \n",
      " 8   previous_3_month_share              4698 non-null   float64       \n",
      " 9   previous_1_month_consumption        4698 non-null   float64       \n",
      " 10  previous_2_month_consumption        4698 non-null   float64       \n",
      " 11  previous_3_month_consumption        4698 non-null   float64       \n",
      " 12  quarter                             4698 non-null   int64         \n",
      " 13  covid                               4698 non-null   int64         \n",
      " 14  school_holiday                      4698 non-null   int64         \n",
      " 15  2017_nufüs                          4698 non-null   int64         \n",
      " 16  2023_nüfus_prediction               4698 non-null   int64         \n",
      " 17  Yıllık ortalama \n",
      "nüfus artış hızı   4698 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(11), int64(5), object(1)\n",
      "memory usage: 697.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# # Save feature_engineered_df \n",
    "# feature_engineered_df.to_csv(\"data/feature_engineered_df.csv\")\n",
    "# # Load feature_engineered_df \n",
    "parse_dates = [\"date\"]\n",
    "feature_engineered_df = pd.read_csv('data/feature_engineered_df.csv', index_col=[0], parse_dates=parse_dates)\n",
    "feature_engineered_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Split Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    1    2 ... 4290 4291 4292] TEST: [4293 4294 4295 4296 4297 4298 4299 4300 4301 4302 4303 4304 4305 4306\n",
      " 4307 4308 4309 4310 4311 4312 4313 4314 4315 4316 4317 4318 4319 4320\n",
      " 4321 4322 4323 4324 4325 4326 4327 4328 4329 4330 4331 4332 4333 4334\n",
      " 4335 4336 4337 4338 4339 4340 4341 4342 4343 4344 4345 4346 4347 4348\n",
      " 4349 4350 4351 4352 4353 4354 4355 4356 4357 4358 4359 4360 4361 4362\n",
      " 4363 4364 4365 4366 4367 4368 4369 4370 4371 4372 4373]\n",
      "TRAIN: [   0    1    2 ... 4371 4372 4373] TEST: [4374 4375 4376 4377 4378 4379 4380 4381 4382 4383 4384 4385 4386 4387\n",
      " 4388 4389 4390 4391 4392 4393 4394 4395 4396 4397 4398 4399 4400 4401\n",
      " 4402 4403 4404 4405 4406 4407 4408 4409 4410 4411 4412 4413 4414 4415\n",
      " 4416 4417 4418 4419 4420 4421 4422 4423 4424 4425 4426 4427 4428 4429\n",
      " 4430 4431 4432 4433 4434 4435 4436 4437 4438 4439 4440 4441 4442 4443\n",
      " 4444 4445 4446 4447 4448 4449 4450 4451 4452 4453 4454]\n",
      "TRAIN: [   0    1    2 ... 4452 4453 4454] TEST: [4455 4456 4457 4458 4459 4460 4461 4462 4463 4464 4465 4466 4467 4468\n",
      " 4469 4470 4471 4472 4473 4474 4475 4476 4477 4478 4479 4480 4481 4482\n",
      " 4483 4484 4485 4486 4487 4488 4489 4490 4491 4492 4493 4494 4495 4496\n",
      " 4497 4498 4499 4500 4501 4502 4503 4504 4505 4506 4507 4508 4509 4510\n",
      " 4511 4512 4513 4514 4515 4516 4517 4518 4519 4520 4521 4522 4523 4524\n",
      " 4525 4526 4527 4528 4529 4530 4531 4532 4533 4534 4535]\n",
      "TRAIN: [   0    1    2 ... 4533 4534 4535] TEST: [4536 4537 4538 4539 4540 4541 4542 4543 4544 4545 4546 4547 4548 4549\n",
      " 4550 4551 4552 4553 4554 4555 4556 4557 4558 4559 4560 4561 4562 4563\n",
      " 4564 4565 4566 4567 4568 4569 4570 4571 4572 4573 4574 4575 4576 4577\n",
      " 4578 4579 4580 4581 4582 4583 4584 4585 4586 4587 4588 4589 4590 4591\n",
      " 4592 4593 4594 4595 4596 4597 4598 4599 4600 4601 4602 4603 4604 4605\n",
      " 4606 4607 4608 4609 4610 4611 4612 4613 4614 4615 4616]\n",
      "TRAIN: [   0    1    2 ... 4614 4615 4616] TEST: [4617 4618 4619 4620 4621 4622 4623 4624 4625 4626 4627 4628 4629 4630\n",
      " 4631 4632 4633 4634 4635 4636 4637 4638 4639 4640 4641 4642 4643 4644\n",
      " 4645 4646 4647 4648 4649 4650 4651 4652 4653 4654 4655 4656 4657 4658\n",
      " 4659 4660 4661 4662 4663 4664 4665 4666 4667 4668 4669 4670 4671 4672\n",
      " 4673 4674 4675 4676 4677 4678 4679 4680 4681 4682 4683 4684 4685 4686\n",
      " 4687 4688 4689 4690 4691 4692 4693 4694 4695 4696 4697]\n"
     ]
    }
   ],
   "source": [
    "deneme = feature_engineered_df.sort_values(by=[\"date\"]).reset_index(drop=True).copy()\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=81)\n",
    "cv = tscv\n",
    "for train_index, test_index in tscv.split(deneme):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = feature_engineered_df.sort_values(by=[\"date\"]).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.model_selection.data_preperation import DataPreperation\n",
    "\n",
    "# Initialize Data Preperation module\n",
    "# dp = DataPreperation(data=model_df)\n",
    "# # Train-Test split\n",
    "# X_train, y_train, X_test, y_test = dp.tts_last_month(index_column1=\"date\" ,index_column2=\"province\")\n",
    "\n",
    "# latter\n",
    "# X_train, y_train, X_test, y_test, X_val, y_val = dp.ttvs_last_month(index_column1=\"date\" ,index_column2=\"province\",lag=lag, split_size = 0.05)\n",
    "# print(\"Maximum date at val is: \", X_test.index.max(), \" Shape is: \", X_val.shape)\n",
    "# print(\"Minimum date at val is: \", X_test.index.min(), \" Shape is: \", X_val.shape)  \n",
    "# print(\"Maximum date at train is: \", X_train.index.max(),\" Shape is: \", X_train.shape)\n",
    "# print(\"Minimum date at train is: \", X_train.index.min(),\" Shape is: \", X_train.shape)\n",
    "# print(\"Maximum date at test is: \", X_test.index.max(), \" Shape is: \", X_test.shape)\n",
    "# print(\"Minimum date at test is: \", X_test.index.min(), \" Shape is: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error \n",
    "def rmse(actual_value: Any, prediction: Any):\n",
    "    return np.sqrt(((actual_value - prediction) ** 2)/2)\n",
    "\n",
    "def ape(actual_value: Any, prediction: Any):\n",
    "    return abs(actual_value-prediction)/actual_value*100\n",
    "     \n",
    "def prep_scoring_df(X_test: pd.DataFrame, y_test: pd.DataFrame, target_col: str=\"current_month_consumption\", col_list: list=None)-> pd.DataFrame:\n",
    "    scoring_df = X_test.copy()\n",
    "    scoring_df[\"actual_value\"] = y_test[target_col]\n",
    "    scoring_df.reset_index(level=\"province\",inplace=True)\n",
    "    if not col_list:\n",
    "        col_list = [\"province\", \"actual_value\"]\n",
    "    scoring_df = scoring_df[col_list]\n",
    "    return scoring_df\n",
    "\n",
    "def scoring(X_test: pd.DataFrame, y_test: pd.DataFrame, scoring_df: pd.DataFrame, predictions: Any, model_name: str=\"XGB\")-> pd.DataFrame:\n",
    "    if scoring_df.empty:\n",
    "        scoring_df = prep_scoring_df(X_test=X_test, y_test=y_test)\n",
    "    else: \n",
    "        scoring_df = scoring_df.copy()\n",
    "    # Insert predictions\n",
    "    scoring_df[f\"{model_name}_prediction\"] = predictions\n",
    "    # Add rmse\n",
    "    scoring_df[f\"{model_name}_rmse\"] = scoring_df.apply(lambda row: rmse(row.actual_value, row[f\"{model_name}_prediction\"]), axis=1)\n",
    "    # Add percentage difference\n",
    "    scoring_df[f\"{model_name}_absolute_percent_error\"] = scoring_df.apply(lambda row: ape(row.actual_value, row[f\"{model_name}_prediction\"]), axis=1)\n",
    "    \n",
    "    print(\"Number of records greater than 20 percent error:\",(scoring_df[f\"{model_name}_absolute_percent_error\"] > 20).sum())\n",
    "    print(\"Number of records greater than 15 percent error:\",(scoring_df[f\"{model_name}_absolute_percent_error\"] > 15).sum())\n",
    "    print(\"Number of records greater than 10 percent error:\",(scoring_df[f\"{model_name}_absolute_percent_error\"] > 10).sum())\n",
    "    print(\"Number of records greater than 5 percent error:\",(scoring_df[f\"{model_name}_absolute_percent_error\"] > 5).sum())\n",
    "    return scoring_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization parameters\n",
    "init_params = {\n",
    "    # \"early_stopping_rounds\": 20,\n",
    "    # \"eval_metric\": [\"rmsle\",\"mape\",\"rmse\"] # default ,mae(kötü) ,mape(kötü), rmse, rmsle(1 tane 54 gerisi 20nin altında), \n",
    "}\n",
    "\n",
    "# Grid Search Parameters\n",
    "# grid_search_params = {\n",
    "#     'alpha': [0.005, 0.01, 0.015],    \n",
    "#     'colsample_bytree': [1.0],\n",
    "#     'learning_rate': [0.045, 0.050],    \n",
    "#     'max_depth': [7, 8, 9],\n",
    "#     'min_child_weight': [2, 3],\n",
    "#     'n_estimators': [400, 450, 500],\n",
    "#     'subsample': [1],\n",
    "# }\n",
    "grid_search_params = {\n",
    "    'alpha': [0.005, 0.0075, 0.01, 0.0125, 0.015],    \n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'learning_rate': [0.045, 0.050],    \n",
    "    'max_depth': [6, 7, 8],\n",
    "    'min_child_weight': [1,2, 3],\n",
    "    'n_estimators': [400, 450, 500, 550, 1000],\n",
    "    'subsample': [0.5, 1],\n",
    "}\n",
    "fit_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run for Lag= 0\n",
      "Fitting 5 folds for each of 1800 candidates, totalling 9000 fits\n",
      "Best Parameters when lag = 0 {'alpha': 0.015, 'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 8, 'min_child_weight': 2, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "Number of records greater than 20 percent error: 5\n",
      "Number of records greater than 15 percent error: 9\n",
      "Number of records greater than 10 percent error: 12\n",
      "Number of records greater than 5 percent error: 33\n",
      "Run for Lag= 1\n",
      "Fitting 5 folds for each of 1800 candidates, totalling 9000 fits\n",
      "Best Parameters when lag = 1 {'alpha': 0.0075, 'colsample_bytree': 1.0, 'learning_rate': 0.045, 'max_depth': 8, 'min_child_weight': 1, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "Number of records greater than 20 percent error: 1\n",
      "Number of records greater than 15 percent error: 1\n",
      "Number of records greater than 10 percent error: 2\n",
      "Number of records greater than 5 percent error: 4\n",
      "Run for Lag= 2\n",
      "Fitting 5 folds for each of 1800 candidates, totalling 9000 fits\n",
      "Best Parameters when lag = 2 {'alpha': 0.0075, 'colsample_bytree': 1.0, 'learning_rate': 0.045, 'max_depth': 8, 'min_child_weight': 1, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "Number of records greater than 20 percent error: 0\n",
      "Number of records greater than 15 percent error: 0\n",
      "Number of records greater than 10 percent error: 0\n",
      "Number of records greater than 5 percent error: 4\n"
     ]
    }
   ],
   "source": [
    "from src.model_selection.data_preperation import DataPreperation\n",
    "from src.model_selection.modelling import xgb_simulator\n",
    "\n",
    "for lag in range(3):\n",
    "    print(\"Run for Lag=\", lag)\n",
    "    # Train test split\n",
    "    dp = DataPreperation(data=model_df)\n",
    "    X_train, y_train, X_test, y_test = dp.tts_last_month(index_column1=\"date\" ,index_column2=\"province\", lag=lag)\n",
    "\n",
    "    # Initialize Simulator\n",
    "    xgb = xgb_simulator(init_params=init_params, fit_params=fit_params, \n",
    "                        grid_search_params=grid_search_params, save=False)\n",
    "    \n",
    "    # Train the model\n",
    "    xgb(X_train=X_train, y_train=y_train)\n",
    "    # Set the best_model and best_parameter\n",
    "    best_model = xgb.best_model\n",
    "    best_params = xgb.best_params\n",
    "    print(f\"Best Parameters when lag = {lag}\",best_params)\n",
    "    # Predict test set\n",
    "    predictons = best_model.predict(X_test)\n",
    "    # Log results\n",
    "    scoring_df=pd.DataFrame()\n",
    "    scoring_df = scoring(X_test=X_test, y_test=y_test, predictions=predictons, scoring_df=scoring_df, model_name=f\"Lag_{lag}_XGB_\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'With cv = timseries.split'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"With cv = timseries.split\"\"\"\n",
    "# Run for Lag= 0\n",
    "# Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
    "# Best Parameters when lag = 0 {'alpha': 0.005, 'colsample_bytree': 1.0, 'learning_rate': 0.045, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 500, 'subsample': 1}\n",
    "# Number of records greater than 20 percent error: 6\n",
    "# Number of records greater than 15 percent error: 9\n",
    "# Number of records greater than 10 percent error: 16\n",
    "# Number of records greater than 5 percent error: 37\n",
    "# Run for Lag= 1\n",
    "# Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
    "# Best Parameters when lag = 1 {'alpha': 0.01, 'colsample_bytree': 1.0, 'learning_rate': 0.045, 'max_depth': 8, 'min_child_weight': 3, 'n_estimators': 500, 'subsample': 1}\n",
    "# Number of records greater than 20 percent error: 1\n",
    "# Number of records greater than 15 percent error: 1\n",
    "# Number of records greater than 10 percent error: 2\n",
    "# Number of records greater than 5 percent error: 10\n",
    "# Run for Lag= 2\n",
    "# Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
    "# Best Parameters when lag = 2 {'alpha': 0.01, 'colsample_bytree': 1.0, 'learning_rate': 0.045, 'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 500, 'subsample': 1}\n",
    "# Number of records greater than 20 percent error: 0\n",
    "# Number of records greater than 15 percent error: 0\n",
    "# Number of records greater than 10 percent error: 2\n",
    "# Number of records greater than 5 percent error: 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is :  (81, 5)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'XGB_absolute_percent_error'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'XGB_absolute_percent_error'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PC\\diesel_consumption_prediction\\modelling.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/PC/diesel_consumption_prediction/modelling.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplotting\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_metrics\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/PC/diesel_consumption_prediction/modelling.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plot_metrics(scoring_df, col_x\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mprovince\u001b[39;49m\u001b[39m\"\u001b[39;49m, col_y1\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mXGB_absolute_percent_error\u001b[39;49m\u001b[39m\"\u001b[39;49m, col_y2\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mXGB_absolute_percent_error\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\diesel_consumption_prediction\\src\\utils\\plotting.py:30\u001b[0m, in \u001b[0;36mplot_metrics\u001b[1;34m(df, col_x, col_y1, col_y2)\u001b[0m\n\u001b[0;32m     28\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m25\u001b[39m,\u001b[39m5\u001b[39m))\n\u001b[0;32m     29\u001b[0m x \u001b[39m=\u001b[39m df_[col_x]\n\u001b[1;32m---> 30\u001b[0m y1 \u001b[39m=\u001b[39m df_[col_y1]\n\u001b[0;32m     31\u001b[0m y2 \u001b[39m=\u001b[39m df_[col_y2]\n\u001b[0;32m     32\u001b[0m \u001b[39m# Plot x and y\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'XGB_absolute_percent_error'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.utils.plotting import plot_metrics\n",
    "plot_metrics(scoring_df, col_x=\"province\", col_y1=\"XGB_absolute_percent_error\", col_y2=\"XGB_absolute_percent_error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e593ac106456af50ce7af38f9671c411b49d6cd90f9b885e167f0f594e09038c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
