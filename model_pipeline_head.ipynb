{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') \n",
    "import os\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"DataLoad\"\"\"\n",
    "# from src.utils.config import get_config\n",
    "# from src.db_ops.db_ops import PosgreOps\n",
    "\n",
    "# # Load Config\n",
    "# config = get_config()\n",
    "# # Initialize database operator, this case posgre operator\n",
    "# pgops = PosgreOps(config=config)\n",
    "# ### Define the required parameters to load the data ###\n",
    "# # -------------------------------------------------------- #\n",
    "# months = 84\n",
    "# table_name = \"daas.epdk_petrol_province\"\n",
    "# # -------------------------------------------------------- #\n",
    "# # Fetch data from database\n",
    "# epdk_petrol_province_data = pgops.get_monthly_data(table_name=table_name,\n",
    "#                                                 today=date.today(), months=months)\n",
    "# # Close the connection after obtaining the data\n",
    "# pgops.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5904 entries, 0 to 5903\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   date            5904 non-null   object \n",
      " 1   province        5904 non-null   object \n",
      " 2   gasolene_types  5904 non-null   float64\n",
      " 3   diesel_types    5904 non-null   float64\n",
      " 4   fuel_oil_types  5890 non-null   float64\n",
      " 5   aviation_fuels  5875 non-null   float64\n",
      " 6   marine_fuels    5826 non-null   float64\n",
      " 7   paraffin_oil    5838 non-null   float64\n",
      " 8   other_types     5824 non-null   float64\n",
      " 9   total           5904 non-null   float64\n",
      " 10  percentage      5904 non-null   float64\n",
      "dtypes: float64(9), object(2)\n",
      "memory usage: 553.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load Raw data\n",
    "epdk_petrol_province_data = pd.read_csv('data/daas_raw_data.csv', index_col=[0])\n",
    "epdk_petrol_province_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>province</th>\n",
       "      <th>gasolene_types</th>\n",
       "      <th>diesel_types</th>\n",
       "      <th>fuel_oil_types</th>\n",
       "      <th>aviation_fuels</th>\n",
       "      <th>marine_fuels</th>\n",
       "      <th>paraffin_oil</th>\n",
       "      <th>other_types</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>İSTANBUL</td>\n",
       "      <td>64029877.00</td>\n",
       "      <td>309312242.0</td>\n",
       "      <td>227.04</td>\n",
       "      <td>35862161.00</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.095813e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>İSTANBUL</td>\n",
       "      <td>56566776.00</td>\n",
       "      <td>296383914.0</td>\n",
       "      <td>120.82</td>\n",
       "      <td>27658.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.807452e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>İSTANBUL</td>\n",
       "      <td>36589366.00</td>\n",
       "      <td>217554409.0</td>\n",
       "      <td>216.26</td>\n",
       "      <td>16877432.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11771.00</td>\n",
       "      <td>13241.0</td>\n",
       "      <td>2.712536e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>İSTANBUL</td>\n",
       "      <td>42943407.00</td>\n",
       "      <td>261733757.0</td>\n",
       "      <td>334.94</td>\n",
       "      <td>20745681.00</td>\n",
       "      <td>200.0</td>\n",
       "      <td>15738.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.259735e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>2021-01-03</td>\n",
       "      <td>İSTANBUL</td>\n",
       "      <td>51458.42</td>\n",
       "      <td>292829027.0</td>\n",
       "      <td>384.59</td>\n",
       "      <td>22623562.00</td>\n",
       "      <td>370.0</td>\n",
       "      <td>14327.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.676799e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  province  gasolene_types  diesel_types  fuel_oil_types  \\\n",
       "0    2021-01-07  İSTANBUL     64029877.00   309312242.0          227.04   \n",
       "82   2021-01-06  İSTANBUL     56566776.00   296383914.0          120.82   \n",
       "164  2021-01-05  İSTANBUL     36589366.00   217554409.0          216.26   \n",
       "246  2021-01-04  İSTANBUL     42943407.00   261733757.0          334.94   \n",
       "328  2021-01-03  İSTANBUL        51458.42   292829027.0          384.59   \n",
       "\n",
       "     aviation_fuels  marine_fuels  paraffin_oil  other_types         total  \n",
       "0       35862161.00         150.0          0.00          0.0  4.095813e+08  \n",
       "82         27658.03           0.0         15.67          0.0  3.807452e+05  \n",
       "164     16877432.00           0.0      11771.00      13241.0  2.712536e+08  \n",
       "246     20745681.00         200.0      15738.00          0.0  3.259735e+08  \n",
       "328     22623562.00         370.0      14327.00          0.0  3.676799e+08  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epdk_petrol_province_data.query(\"province == 'İSTANBUL'\").drop(columns=[\"percentage\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Dates are:  [numpy.datetime64('2021-10-01T00:00:00.000000000')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Preprocessing\"\"\"\n",
    "from src.preprocessing.preprocess import PreprocessData\n",
    "\n",
    "# Initialize preprocess object\n",
    "prep = PreprocessData(data=epdk_petrol_province_data)\n",
    "### Define parameters to preprocess ###\n",
    "# -------------------------------------------------------- #\n",
    "target_col_list = [\"date\",\"province\",\"diesel_types\"]\n",
    "row_drop_dict = {\"province\": \"Toplam\"} \n",
    "# format_date_flag = True # True by default\n",
    "col_rename_dict = {\"diesel_types\":\"current_month_consumption\"}\n",
    "anomaly_col = \"current_month_consumption\" \n",
    "# -------------------------------------------------------- #\n",
    "# Preprocess data with given parameters\n",
    "preprocessed_df = prep.preprocess_data(target_col_list=target_col_list,\n",
    "                                 row_drop_dict=row_drop_dict,\n",
    "                                 col_rename_dict=col_rename_dict,\n",
    "                                 anomaly_col=anomaly_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\02485955\\bireysel_proje\\env38\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "c:\\Users\\02485955\\bireysel_proje\\env38\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "c:\\Users\\02485955\\bireysel_proje\\env38\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "c:\\Users\\02485955\\bireysel_proje\\env38\\lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-invertible starting MA parameters found.'\n",
      "02:44:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:44 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:44:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:44:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:19 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:21 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:22 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:23 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "02:45:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:45:28 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'province', 'current_month_consumption', 'trend', 'yhat_lower',\n",
      "       'yhat_upper', 'yhat'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FeatureEngineering\"\"\"\n",
    "from src.feature_engineering.feat_eng import FeatureEngineering\n",
    "\n",
    "# Initialize feature engineering object\n",
    "feng = FeatureEngineering(data=preprocessed_df)\n",
    "# Apply feature engineering to the preprocessed data\n",
    "feature_engineered_df = feng.feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_engineered_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PC\\bireysel_proje\\model_pipeline_head.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/PC/bireysel_proje/model_pipeline_head.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m feature_engineered_df\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_engineered_df' is not defined"
     ]
    }
   ],
   "source": [
    "feature_engineered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5555333333333333, 1.0, 0.0, 0.11113333333333332]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "b = []\n",
    "for i in a:\n",
    "    new_val = (i-min(a))/(max(a)-min(a))\n",
    "    # new_val *= \n",
    "    b.append(new_val)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.878787878787877"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import minmax_scale, scale\n",
    "scores = [0.12, 0.12, 0.03, 0.024]\n",
    "preds = [13, 11, 8, 7]\n",
    "normalized = minmax_scale(scores)\n",
    "deneme = np.multiply(preds,normalized)\n",
    "deneme.sum()/normalized.sum() # this value is expected to be closer to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.000000000000002"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(12*13+12*13+3*8+7*2.4)/(12+12+3+2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3821656050955414"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12/31.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized = scale(scores)\n",
    "\n",
    "nor_st = scale(normalized)\n",
    "st_nor = minmax_scale(standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.142863265262391"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.14286327, 1.        , 0.        ])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.39992800575954"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneme = np.multiply(preds,normalized)\n",
    "deneme.sum()/normalized.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = {}\n",
    "trial[0] = np.multiply(preds,st_nor)\n",
    "trial[1] = np.multiply(preds,standardized)\n",
    "trial[2] = np.multiply(preds,normalized)\n",
    "trial[3] = np.multiply(preds,nor_st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.999866666666666\n",
      "6.948210452673974\n",
      "18.999866666666666\n",
      "6.9482104526739725\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,4):\n",
    "    print(trial[i].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "deneme = scale(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.35088059,  1.47393429, -1.0528102 , -0.77200467])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55553333, 1.        , 0.        , 0.11113333])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax_scale(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_a = pd.Series(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 3.333 10.    -5.    -3.333].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\PC\\bireysel_proje\\model_pipeline_head.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/PC/bireysel_proje/model_pipeline_head.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sc\u001b[39m.\u001b[39;49mtransform(a)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:496\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39m\"\"\"Scale features of X according to feature_range.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \n\u001b[0;32m    484\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39m    Transformed data.\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    494\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 496\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    497\u001b[0m     X,\n\u001b[0;32m    498\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy,\n\u001b[0;32m    499\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    500\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    501\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    502\u001b[0m )\n\u001b[0;32m    504\u001b[0m X \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_\n\u001b[0;32m    505\u001b[0m X \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:769\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    768\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 769\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    770\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    771\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    772\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    773\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    774\u001b[0m         )\n\u001b[0;32m    776\u001b[0m \u001b[39m# make sure we actually converted to numeric:\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mOUSV\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 3.333 10.    -5.    -3.333].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "sc.transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.999866666666666"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "(np.array(b)*np.array(preds)).sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e593ac106456af50ce7af38f9671c411b49d6cd90f9b885e167f0f594e09038c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
