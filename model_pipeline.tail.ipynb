{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4698 entries, 15 to 5912\n",
      "Data columns (total 22 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   date                         4698 non-null   datetime64[ns]\n",
      " 1   province                     4698 non-null   object        \n",
      " 2   current_month_consumption    4698 non-null   float64       \n",
      " 3   ARIMA_prediction             4698 non-null   float64       \n",
      " 4   last_year_total_consumption  4698 non-null   float64       \n",
      " 5   rolling_mean_2               4698 non-null   float64       \n",
      " 6   rolling_mean_3               4698 non-null   float64       \n",
      " 7   lag1_monthly_share           4698 non-null   float64       \n",
      " 8   lag2_monthly_share           4698 non-null   float64       \n",
      " 9   lag3_monthly_share           4698 non-null   float64       \n",
      " 10  lag1                         4698 non-null   float64       \n",
      " 11  lag2                         4698 non-null   float64       \n",
      " 12  lag3                         4698 non-null   float64       \n",
      " 13  quarter                      4698 non-null   int64         \n",
      " 14  month                        4698 non-null   int64         \n",
      " 15  covid                        4698 non-null   int64         \n",
      " 16  school_holiday               4698 non-null   int64         \n",
      " 17  population                   4698 non-null   float64       \n",
      " 18  trend                        4698 non-null   float64       \n",
      " 19  yhat_lower                   4698 non-null   float64       \n",
      " 20  yhat_upper                   4698 non-null   float64       \n",
      " 21  yhat                         4698 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(16), int64(4), object(1)\n",
      "memory usage: 844.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# # Save feature_engineered_df \n",
    "# feature_engineered_df.to_csv(\"data/feature_engineered_df.csv\")\n",
    "# # Load feature_engineered_df \n",
    "parse_dates = [\"date\"]\n",
    "feature_engineered_df = pd.read_csv('data/feature_engineered_df.csv', index_col=[0], parse_dates=parse_dates)\n",
    "feature_engineered_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultimate Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range date at train is:  2017-04-01 00:00:00 2021-11-01 00:00:00 with shape of:  (4536, 21)\n",
      "Date range date at test is:  2021-12-01 00:00:00 2021-12-01 00:00:00 with shape of:  (81, 21)\n"
     ]
    }
   ],
   "source": [
    "from src.model_selection.data_preperation import DataPreperation as dp\n",
    "model_df = feature_engineered_df.sort_values(by=[\"date\"]).reset_index(drop=True).copy()\n",
    "main_train, main_test = dp.train_test_split(data=model_df, index_column1=\"date\",index_column2=\"province\",lag=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range date at train is:  2017-04-01 00:00:00 2021-10-01 00:00:00 with shape of:  (4455, 21)\n",
      "Date range date at test is:  2021-11-01 00:00:00 2021-11-01 00:00:00 with shape of:  (81, 21)\n",
      "Maximum date at train is:  (Timestamp('2021-10-01 00:00:00'), 'ŞIRNAK')  Shape is:  (4455, 19)\n",
      "Minimum date at train is:  (Timestamp('2017-04-01 00:00:00'), 'ADANA')  Shape is:  (4455, 19)\n",
      "Maximum date at test is:  (Timestamp('2021-11-01 00:00:00'), 'ŞIRNAK')  Shape is:  (81, 19)\n",
      "Minimum date at test is:  (Timestamp('2021-11-01 00:00:00'), 'ADANA')  Shape is:  (81, 19)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = dp.ts_train_test_split(data=main_train.reset_index(),\n",
    "                                                          index_column1=\"date\",\n",
    "                                                          index_column2=\"province\",\n",
    "                                                          lag=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cbt\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index_column1': 'date',\n",
       " 'index_column2': 'province',\n",
       " 'lag': 0,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_pipeline_params = {\n",
    "    \"estimator_list\":[\"xgboost\",\"lightgbm\",\"catboost\"],\n",
    "    \"estimator_params\": {\n",
    "        \"init_params\":\n",
    "            {\n",
    "            \"xgboost__init_params\": {\"objective\":\"reg:gamma\"},\n",
    "            \"lightgbm__init_params\": {\"objective\":\"gamma\"},\n",
    "            \"catboost__init_params\": {}\n",
    "            },\n",
    "        \"grid_search_params\":\n",
    "            {\n",
    "            \"xgboost__grid_search_params\": {\n",
    "                'alpha': [i for i in np.linspace(0.0,0.01, num=4)], # Defaults to 0\n",
    "                'colsample_bytree': [0.8, 0.9, 1], # Defaults to 1\n",
    "                'lambda': [i for i in np.linspace(0.33,1,num=4)], # Defaults to 1\n",
    "                'learning_rate': [i for i in np.linspace(0.075,0.225, num=6)], # Defaults to 0.3\n",
    "                'max_depth': [6, 8, 10], # Defaults to 6\n",
    "                'min_child_weight': [1,2,3,4], # Defaults to 1\n",
    "                'n_estimators': [150,200,250], \n",
    "                'subsample': [0.5,1], # Defaults to 1\n",
    "                },\n",
    "            \"lightgbm__grid_search_params\": {\n",
    "                'num_iterations': [100, 250, 500],\n",
    "                'num_leaves':[20,31,50],\n",
    "                'max_depth':[-1,8,10],\n",
    "                'learning_rate':[i for i in np.linspace(0.01,0.15,num=4)],\n",
    "                'min_data_in_leaf':[15, 20, 25],\n",
    "                'min_child_samples':[5,10,15],\n",
    "                'feature_fraction': [0.25, 0.5, 1],\n",
    "                'bagging_fraction': [i for i in np.linspace(0,1,num=4)],\n",
    "                'bagging_freq': [50, 75, 100],\n",
    "                },\n",
    "            \"catboost__grid_search_params\":{\n",
    "                \n",
    "            }        \n",
    "            },\n",
    "        \"fit_params\": \n",
    "            {\n",
    "            \"xgboost__fit_params\":{},\n",
    "            \"lightgbm__fit_params\":{},\n",
    "            \"catboost__fit_params\":{}                        \n",
    "            }\n",
    "    },\n",
    "    \"cross_validation_params\": {\n",
    "        \"validator\": \"time_series\",\n",
    "        \"k\": 9,\n",
    "        \"test_size\": 81,\n",
    "        \"gap\": 0\n",
    "    },\n",
    "    \n",
    "    \"train_test_split\":{\"index_column1\":\"date\",\n",
    "                           \"index_column2\":\"province\",\n",
    "                           \"lag\":0, \n",
    "                           \"verbose\": False},\n",
    "    \"hp_optimizer\": {\n",
    "        \"optimizer_type\": \"RandomSearchCV\",\n",
    "        \"scoring\":\"neg_mean_absolute_percantage_error\",\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "}\n",
    "gs_pipeline_params[\"train_test_split\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range date at train is:  2017-04-01 00:00:00 2021-10-01 00:00:00 with shape of:  (4455, 21)\n",
      "Date range date at test is:  2021-11-01 00:00:00 2021-11-01 00:00:00 with shape of:  (81, 21)\n",
      "Estimators are: ['xgboost', 'lightgbm', 'catboost']\n",
      "{'xgboost__init_params': {'objective': 'reg:gamma'}, 'lightgbm__init_params': {'objective': 'gamma'}, 'catboost__init_params': {}}\n",
      "Estimator is xgboost\n",
      "{'optimizer_type': 'RandomSearchCV', 'scoring': 'neg_mean_absolute_percantage_error', 'n_jobs': -1}\n",
      "Estimator is lightgbm\n",
      "{'optimizer_type': 'RandomSearchCV', 'scoring': 'neg_mean_absolute_percantage_error', 'n_jobs': -1}\n",
      "Estimator is catboost\n",
      "{'optimizer_type': 'RandomSearchCV', 'scoring': 'neg_mean_absolute_percantage_error', 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "def grid_search_pipeline(data: pd.DataFrame, estimator_list: list,\n",
    "                        estimator_params: dict,\n",
    "                        scoring: str=\"neg_mean_absolute_percantage_error\", **kwargs):\n",
    "    # Train Test Split the data for Grid Search\n",
    "    X_train, y_train, X_test, y_test = dp.ts_train_test_split(data=data.reset_index(),\n",
    "                                                            **kwargs[\"train_test_split\"])\n",
    "    # Establish Estimators pipeline\n",
    "    estimator_pipeline = make_estimator_pipeline(estimator_list=estimator_list, **estimator_params[\"init_params\"])\n",
    "    print(estimator_params[\"init_params\"])\n",
    "    # Initialize the Cross Validator\n",
    "    cv = get_cross_validator(**kwargs[\"cross_validation_params\"])\n",
    "    # Perform Random Search and Store Results\n",
    "    for est in estimator_list:\n",
    "        print(\"Estimator is\",est)\n",
    "        grid = hp_optimizer(estimator=estimator_pipeline[est], cv=cv, **kwargs[\"hp_optimizer\"]).fit(X=X_train, y=y_train, **estimator_params[\"fit_params\"][f\"{est}__fit_params\"])\n",
    "        \n",
    "        \n",
    "def hp_optimizer(estimator: Any, search_params:dict, cv:Any, optimizer_type:str, scoring:str, n_jobs:int):\n",
    "    if \"GridSearchCV\" in optimizer_type:\n",
    "        return GridSearchCV(\n",
    "                        estimator=estimator, # Target Estimator\n",
    "                        param_distributions=search_params, # Hyperparameters set\n",
    "                        cv=cv, # Cross Validator\n",
    "                        n_jobs=n_jobs, # Number of parallel jobs\n",
    "                        scoring=scoring, # Scoring parameter\n",
    "                        Verbose=True\n",
    "                        )\n",
    "    elif \"RandomSearchCV\" in optimizer_type:\n",
    "        return RandomizedSearchCV(\n",
    "                        estimator=estimator, # Target Estimator\n",
    "                        param_distributions=search_params, # Hyperparameters set\n",
    "                        cv=cv, # Cross Validator\n",
    "                        n_jobs=n_jobs, # Number of parallel jobs\n",
    "                        scoring=scoring, # Scoring parameter\n",
    "                        )    \n",
    "        \n",
    "def make_estimator_pipeline(estimator_list: list, **kwargs):\n",
    "    estimator_pipeline = dict()\n",
    "    print(\"Estimators are:\",estimator_list)\n",
    "    # print(kwargs)\n",
    "    if \"xgboost\" in estimator_list:\n",
    "        estimator_pipeline[\"xgboost\"] = xgb.XGBRegressor(**kwargs[\"xgboost__init_params\"])\n",
    "        # print(kwargs[\"xgboost__init_params\"])\n",
    "    if \"lightgbm\" in estimator_list:\n",
    "        estimator_pipeline[\"lightgbm\"] = lgb.LGBMRegressor(**kwargs[\"lightgbm__init_params\"])\n",
    "        # print(kwargs[\"lightgbm__init_params\"])\n",
    "    if \"catboost\" in estimator_list:\n",
    "        estimator_pipeline[\"catboost\"] = cbt.CatBoostRegressor(**kwargs[\"catboost__init_params\"])\n",
    "        # print(kwargs[\"catboost__init_params\"])\n",
    "    return estimator_pipeline\n",
    "\n",
    "def get_cross_validator(validator: str, k: int=9, test_size: int=81, gap: int=0):\n",
    "    if validator == \"time_series\":\n",
    "        from sklearn.model_selection import TimeSeriesSplit\n",
    "        # print(\"time_series_cross_val\")\n",
    "        tscv = TimeSeriesSplit(gap=gap, max_train_size=None, n_splits=k, test_size=test_size)\n",
    "        return tscv\n",
    "    else:\n",
    "        print(\"k-fold cross validation\")\n",
    "        return k\n",
    "grid_search_pipeline(data=main_train, **gs_pipeline_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 ('env38': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16f20fb1ed6254620f2d636807022f0ec277b178915348b63e9bfe829f5d63c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
